{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "2b21fbe8dbd8ff7b",
   "cell_type": "markdown",
   "source": "# Japanese Phrases Generator Project\n\nA multi-agent system for generating Japanese phrases across different verb tenses.\n\n## System Overview\nThis notebook implements an intelligent system that leverages multiple AI agents to generate grammatically correct Japanese sentences. The system focuses on verb conjugation and tense variations.\n\n### Key Components\n\n#### Agents\n- **Chatbot Agent**: Manages user interactions and maintains system state\n- **Sentence Expert Agent**: Specialized in Japanese sentence construction and grammar rules\n\n#### Features\n- Dynamic phrase generation\n- Support for multiple verb tenses\n- State management and persistence\n- Modular and extensible architecture\n\n#### Project Structure\n- Tools and utilities for agent interaction\n- State management system\n- Agent implementation modules\n\n# IMPORTANT!\nThe app built in this notebook takes user input using a text box (Python's input). These are commented-out to ensure that you can use the Run all feature without interruption. Keep an eye out for the steps where you need to uncomment the .invoke(...) calls in order to interact with the app.\n",
   "metadata": {}
  },
  {
   "id": "715947f00e0914fb",
   "cell_type": "markdown",
   "source": "# Get set up\nStart by installing and importing the LangGraph SDK and LangChain support for the Gemini API.\nWe will use pydantic do define structured output,\nThe dango package will be used to check if a word is a verb, and to tokenize the generated phrase.\n",
   "metadata": {}
  },
  {
   "id": "3d048c38a3c84117",
   "cell_type": "code",
   "source": [
    "# Remove conflicting packages from the Kaggle base environment.\n",
    "# !pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
    "\n",
    "# Install langgraph and the packages used in this lab.\n",
    "!pip install langchain-core==0.3.52 langgraph==0.3.29 typing_extensions==4.13.2 dango==0.0.1 langchain-google-genai==2.1.2 pydantic==2.11.3"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.877742Z",
     "start_time": "2025-04-23T08:02:41.054595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core==0.3.52 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (0.3.52)\n",
      "Requirement already satisfied: langgraph==0.3.29 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (0.3.29)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (4.13.2)\n",
      "Requirement already satisfied: dango==0.0.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: langchain-google-genai==2.1.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: pydantic==2.11.3 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (2.11.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langchain-core==0.3.52) (0.3.30)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langchain-core==0.3.52) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langchain-core==0.3.52) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langchain-core==0.3.52) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langchain-core==0.3.52) (24.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langgraph==0.3.29) (2.0.24)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langgraph==0.3.29) (0.1.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langgraph==0.3.29) (0.1.61)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langgraph==0.3.29) (3.5.0)\n",
      "Requirement already satisfied: pygtrie~=2.4 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from dango==0.0.1) (2.5.0)\n",
      "Requirement already satisfied: SudachiPy~=0.5.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from dango==0.0.1) (0.5.4)\n",
      "Requirement already satisfied: SudachiDict-core>=20210608 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from dango==0.0.1) (20250129)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langchain-google-genai==2.1.2) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langchain-google-genai==2.1.2) (0.6.17)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from pydantic==2.11.3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from pydantic==2.11.3) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from pydantic==2.11.3) (0.4.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (2.24.2)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (6.30.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.52) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.29) (1.9.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.52) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.52) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.52) (0.23.0)\n",
      "Requirement already satisfied: sortedcontainers~=2.1.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from SudachiPy~=0.5.2->dango==0.0.1) (2.1.0)\n",
      "Requirement already satisfied: dartsclone~=0.9.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from SudachiPy~=0.5.2->dango==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: Cython in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from dartsclone~=0.9.0->SudachiPy~=0.5.2->dango==0.0.1) (3.0.12)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (1.69.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (1.72.0rc1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (1.72.0rc1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (4.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core==0.3.52) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core==0.3.52) (2.4.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.2) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\luca\\miniconda3\\envs\\senseibot\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.29) (1.3.1)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "id": "295d0603dc13f705",
   "cell_type": "markdown",
   "source": "# Set up your API key\nThe GOOGLE_API_KEY environment variable can be set to automatically configure the underlying API. This works for both the official Gemini Python SDK and for LangChain/LangGraph.\n\nTo run the following cell, your API key must be stored it in a Kaggle secret named GOOGLE_API_KEY.\n\nIf you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.\n\nTo make the key available through Kaggle secrets, choose Secrets from the Add-ons menu and follow the instructions to add your key or enable it for this notebook.",
   "metadata": {}
  },
  {
   "id": "7ed351f2bc43f9c7",
   "cell_type": "code",
   "source": [
    "## Uncomment on a Keggle environment\n",
    "#import os\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "#GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "#os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.889512Z",
     "start_time": "2025-04-23T08:02:41.887266Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "id": "1dceeaa3c67e695f",
   "cell_type": "markdown",
   "source": "# Tools for the chatbot\n\nThis set of tools forms the core of the user interaction for collecting the necessary ingredients – Japanese verbs and grammatical tenses – before the system can generate phrases.\nThink of it as the agent's toolkit for managing a virtual \"notebook\" where user selections are recorded.\n",
   "metadata": {}
  },
  {
   "id": "aaec2bc7dcfb32e3",
   "cell_type": "markdown",
   "source": "### List of supported tenses\n\nFirst the chatbot agent can call the `get_verb_tenses()` method. It works as a \"Menu\" from which the user should pick the tense.\n\nThe formatting of the Non-Past Tense and Past Tense parts informs the chatbot that the user should pick between one of the choices and not a generic \"past tense\".",
   "metadata": {}
  },
  {
   "id": "ad80eaa5e7d95d62",
   "cell_type": "code",
   "source": "from langchain_core.tools import tool\n\n\n@tool\ndef get_verb_tenses() -> str:\n    \"\"\"Get a list of all the verb tenses.\"\"\"\n\n    return \"\"\"\n    GRAMMAR:\n    Non-Past Tense:\n    Non-Past Affirmative Plain\n    Non-Past Affirmative Polite\n    Non-Past Negative Plain\n    Non-Past Negative Polite\n\n    Past Tense:\n    Past Affirmative Plain\n    Past Affirmative Polite\n    Past Negative Plain\n    Past Negative Polite\n\n    Te Form\n    Desiderative\n    Volitional\n    Exhortative\n    Imperative\n    Request\n    Potential\n    Passive\n    Causative\n    Conditional\n    \"\"\"",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.903122Z",
     "start_time": "2025-04-23T08:02:41.899839Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "id": "17c5163d1c4e5e03",
   "cell_type": "markdown",
   "source": "### Verifying User Input: The `is_verb` Tool\n\nA critical function of the chatbot is to ensure the words provided by the user are suitable for phrase generation – specifically, that they are Japanese verbs. While the powerful language model underlying the chatbot *could* attempt this check, relying solely on it carries a risk of occasional errors or \"hallucinations.\"\n\nTo guarantee accuracy and predictability, the system employs a dedicated tool: `is_verb`. When the user suggests a word, the chatbot calls this tool. `is_verb` then uses the specialized `dango` Japanese language library to definitively determine if the word's part of speech is actually a verb. This approach provides a reliable, rule-based verification, ensuring only valid verbs are added to the notebook for phrase generation.",
   "metadata": {}
  },
  {
   "id": "d088ccd2d2b09052",
   "cell_type": "code",
   "source": [
    "from dango.word import PartOfSpeech\n",
    "from dango import dango\n",
    "\n",
    "\n",
    "@tool\n",
    "def is_verb(word: str):\n",
    "    \"\"\"Call to verify that a japanese word is a verb.\"\"\"\n",
    "    tokens = dango.tokenize(word)\n",
    "    if len(tokens) != 1:\n",
    "        raise ValueError(\"Input must be a single word.\")\n",
    "    only_word = tokens[0]\n",
    "    return only_word.part_of_speech == PartOfSpeech.VERB"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.920805Z",
     "start_time": "2025-04-23T08:02:41.916194Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "id": "77b476de99a9227d",
   "cell_type": "markdown",
   "source": "## State Management\n\nThe chatbot agent maintains a state object that tracks the conversation and the current state of the system.\n\nThe state object is a dictionary that contains the following keys:\n\n* `messages`: A list of messages sent by the user and received by the chatbot.\n* `verbs`: A list of verbs selected by the user.\n* `tenses`: A list of tenses selected by the user.\n* `generated_phrases`: A list of the sentences generated by the sentence expert agent.",
   "metadata": {}
  },
  {
   "id": "fc3e638b795fe3b8",
   "cell_type": "code",
   "source": "from typing_extensions import TypedDict, Annotated\n\nfrom langgraph.graph import add_messages\n\n\nclass PhraseGeneratorState(TypedDict):\n    \"\"\"State representing the conjugation of a verb.\"\"\"\n\n    # The chat conversation. This preserves the conversation history\n    # between nodes. The add_messages annotation defines how this\n    # state key should be updated (in this case, it appends messages\n    # to the list, rather than overwriting them)\n    messages: Annotated[list, add_messages]\n\n    # The verbs to use to generate the sentences for\n    verbs: list[str]\n\n    # The tense to generate the sentences for\n    tenses: list[str]\n\n    # Flag indicating whether the sentence generation is complete\n    finished: bool\n\n    generated_phrases: list[str]",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.932026Z",
     "start_time": "2025-04-23T08:02:41.928810Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "id": "833322dc60c241b2",
   "cell_type": "markdown",
   "source": "### State Management Tools\n\nThe notebook uses state management tools to track conversation and system state during interactions:\n\n- `add_verb()`: Adds new verbs to the state\n- `clear_verbs()`: Removes all verbs from state\n- `add_tense()`: Adds grammatical tenses to state \n- `clear_tenses()`: Removes all tenses from state\n- `get_notebook()`: Retrieves current state contents\n- `confirm_generation()`: Validates state before phrase generation\n\nBecause LangChain doesn't allow tools to update the state, the void tools allows the LLM to signal wanting to use a certain function,\nbut the state update is effectively made by the collector_node.\n",
   "metadata": {}
  },
  {
   "id": "a4d36477adac0fc9",
   "cell_type": "code",
   "source": "@tool\ndef add_verb(verb: str) -> str:\n    \"\"\"Adds the specified verb to the notebook.\n\n    Returns:\n        The updated verb list.\n    \"\"\"\n\n@tool\ndef clear_verbs():\n    \"\"\"Removes the selected verb from the notebook.\"\"\"\n\n@tool\ndef add_tense(tense: str) -> str:\n    \"\"\"Adds the specified tense to the notebook.\n\n    Returns:\n    The updated verb list.\n    \"\"\"\n\n@tool\ndef clear_tenses():\n    \"\"\"Removes all tenses from the notebook.\"\"\"\n\n@tool\ndef get_notebook():\n    \"\"\"Returns the current elements in the notebook.\"\"\"\n\n@tool\ndef confirm_generation():\n    \"\"\"Confirms the generation of sentences.\"\"\"\n\ndef collector_node(state: PhraseGeneratorState) -> PhraseGeneratorState:\n    \"\"\"The collector node. This is where the phrase generator state is manipulated.\"\"\"\n    tool_msg = state.get(\"messages\", [])[-1]\n    verbs = state.get(\"verbs\", [])\n    tenses = state.get(\"tenses\", [])\n    outbound_msgs = []\n    generation_complete = False\n\n    for tool_call in tool_msg.tool_calls:\n        if tool_call[\"name\"] == \"add_verb\":\n            verbs.append(tool_call[\"args\"][\"verb\"])\n            response = \"\\n\".join(verbs)\n        elif tool_call[\"name\"] == \"clear_verb\":\n            verbs.clear()\n        elif tool_call[\"name\"] == \"add_tense\":\n            tenses.append(tool_call[\"args\"][\"tense\"])\n            response = \"\\n\".join(tenses)\n        elif tool_call[\"name\"] == \"clear_tenses\":\n            tenses.clear()\n        elif tool_call[\"name\"] == \"get_notebook\":\n            response = \"Verbs:\\n\" + \"\\n\".join(verbs) + \"\\n\" + \"Tenses:\\n\" + \"\\n\".join(tenses)\n        elif tool_call[\"name\"] == \"confirm_generation\":\n            print(\"Your notebook:\")\n            print(\"Verbs:\")\n            if not verbs:\n                print(\"   (no verbs)\")\n            for verb in verbs:\n                print(f\"   {verb}\")\n            print(\"Tenses:\")\n            if not tenses:\n                print(\"   (no tenses)\")\n            for tense in tenses:\n                print(f\"   {tense}\")\n            response = input(\"Is this correct? (y/n): \")\n        else:\n            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n\n        outbound_msgs.append(\n            ToolMessage(\n                content=response,\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"]\n            )\n        )\n\n    state = {\n        \"messages\": outbound_msgs,\n        \"verbs\": verbs,\n        \"finished\": False,\n        \"tenses\": tenses,\n        \"generated_phrases\": []\n    }\n\n    return state",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.956428Z",
     "start_time": "2025-04-23T08:02:41.943818Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "id": "503567c74b17b822",
   "cell_type": "markdown",
   "source": "# Sentence Expert Tools: Leveraging Generative AI\n\nUnlike the tools designed for chatbot interactions, our Sentence Expert tools harness the power of generative AI for creating nuanced and contextually relevant sentences.\n\nTo ensure structured and analyzable output, we define a `PhraseResponse` model. This allows us to consistently receive and process the generated sentences.\n\nFurthermore, we employ Few-Shot Prompting with detailed system instructions. This technique provides the generative model with examples, guiding it to produce the desired style and quality of sentences.",
   "metadata": {}
  },
  {
   "id": "362ec61124e7d491",
   "cell_type": "code",
   "source": "from langchain_google_genai import ChatGoogleGenerativeAI\nfrom pydantic import BaseModel, Field\n\n\nclass PhraseResponse(BaseModel):\n    \"\"\"Response model for Japanese phrase generation\"\"\"\n    verb: str = Field(description=\"The Japanese verb used in phrase generation\")\n    tense: str = Field(description=\"The grammatical tense used in phrase generation\")\n    sentence: str = Field(description=\"The generated sentence\")\n\nmodel = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\nllm = model.with_structured_output(PhraseResponse)\n\nGENERATOR_SYSINT = (\n    \"system\",  # 'system' indicates the message is a system instruction.\n    \"You are a Japanese Sentence Generator.\"\n    \"Given a verb and a tense, you generate a simple sentence that must contain the given verb in the given tense.\"\n    \"You cannot do anything else.\"\n    \"The sentence should be simple, with words matching the level of difficulty of the verb.\"\n    \"You should not generate sentences that contains only the verb.\"\n    \"\\n\\n\"\n    \"Here you have some examples:\"\n    \"\"\n    \"example_user: Generate a Japanese sentence using the verb 見る in volitive tense.\"\n    \"example_assistant: 映画を見に行こう。\"\n    \"\"\n       \"\"\n    \"example_user: Generate a Japanese sentence using the verb 写す in past negative polite tense.\"\n    \"example_assistant: 写真を写しませんでした。\"\n    \"\"\n    \"\"\n    \"example_user: Generate a Japanese sentence using the verb 食べる in conditional tense.\"\n    \"example_assistant: 食べたら、元気になります。\"\n    \"\"\n\n    \"\\n\\n\"\n    \"If any of the tools are unavailable, you can break the fourth wall and tell the user that \"\n    \"they have not implemented them yet and should keep reading to do so.\",\n)",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.972448Z",
     "start_time": "2025-04-23T08:02:41.964444Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "id": "d64f253f2429a8da",
   "cell_type": "markdown",
   "source": "Following the established pattern, we create a specific node to streamline the generation process for each unique combination of verb and tense. This focused approach ensures efficient and accurate sentence creation.",
   "metadata": {}
  },
  {
   "id": "9dbc0d8ba81906fb",
   "cell_type": "code",
   "source": "from langchain_core.messages import ToolMessage\n\n\n@tool\ndef generate_sentences():\n    \"\"\"Generate sentences based on the given verb and tense.\"\"\"\n\ndef tokenize_sentence(sentence: str):\n    \"\"\"Tokenize a sentence into words.\"\"\"\n\n    tokens =  dango.tokenize(sentence)\n    result = []\n    for token in tokens:\n        result.append({\n            \"dictionary_form\": token.dictionary_form,\n            \"dictionary_form_reading\": token.dictionary_form_reading,\n            \"part_of_speech\": token.part_of_speech.value,\n            \"surface\": token.surface,\n            \"surface_reading\": token.surface_reading,\n        })\n    return result\n\ndef generator_node(state: PhraseGeneratorState) -> PhraseGeneratorState:\n    \"\"\"The generator node. This is where the phrase generator state is manipulated.\"\"\"\n    tool_msg = state.get(\"messages\", [])[-1]\n    verbs = state.get(\"verbs\", [])\n    tenses = state.get(\"tenses\", [])\n    outbound_msgs = []\n    generated_sentences = []\n    generation_complete = False\n\n    for tool_call in tool_msg.tool_calls:\n        if tool_call[\"name\"] == \"generate_sentences\":\n            for verb in verbs:\n                for tense in tenses:\n                    prompt = f\"Generate a Japanese sentence using the verb '{verb}' in {tense} tense.\"\n                    response = llm.invoke([GENERATOR_SYSINT] + [(\"user\", prompt)])\n                    generated_sentences.append({\n                        \"verb\": response.verb,\n                        \"tense\": response.tense,\n                        \"sentence\": response.sentence,\n                        \"tokenized_sentence\": tokenize_sentence(response.sentence)\n                    })\n\n            if not generated_sentences:\n                response = \"(no sentence generated)\"\n            else:\n                response = \"Generated sentences:\\n\"\n                for sentence in generated_sentences:\n                    response += f\"- {sentence['verb']} ({sentence['tense']}): {sentence['sentence']}\\n\\n\"\n        else:\n            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n\n        print(response)\n\n        outbound_msgs.append(\n            ToolMessage(\n                content=response,\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"]\n            )\n        )\n\n    state = {\n        \"messages\": outbound_msgs,\n        \"verbs\": verbs,\n        \"finished\": generation_complete,\n        \"tenses\": tenses,\n        \"generated_phrases\": generated_sentences\n    }\n\n    return state",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:41.987291Z",
     "start_time": "2025-04-23T08:02:41.981115Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "id": "8b0035554027d18c",
   "cell_type": "markdown",
   "source": "# Handoff tools\n\nFinally the tools that will be used by the agents to transferring conversation to each other.",
   "metadata": {}
  },
  {
   "id": "3116f71700fa351d",
   "cell_type": "code",
   "source": "from langchain_core.messages import ToolMessage\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import InjectedState\nfrom langgraph.types import Command\nfrom typing_extensions import Annotated\n\n@tool\ndef transfer_to_sentence_expert(state: Annotated[dict, InjectedState],\n                                tool_call_id: Annotated[str, InjectedToolCallId]):\n    \"\"\"Transfer the conversation to the sentence expert.\"\"\"\n\n    state['messages'].append(\n        ToolMessage(\n            content=\"Transferring conversation to sentence expert.\",\n            name=\"transfer_to_sentence_expert\",\n            tool_call_id=tool_call_id\n        )\n    )\n\n    return Command(\n        goto=\"sentence_expert\",\n        graph=Command.PARENT,\n        update=state\n    )\n\n@tool\ndef transfer_to_chatbot(state: Annotated[dict, InjectedState],\n                                tool_call_id: Annotated[str, InjectedToolCallId]):\n    \"\"\"Transfer the conversation to the sentence expert.\"\"\"\n\n    state['messages'].append(\n        ToolMessage(\n            content=\"Transferring conversation to chatbot.\",\n            name=\"transfer_to_chatbot\",\n            tool_call_id=tool_call_id\n        )\n    )\n    return Command(\n        goto=\"chatbot\",\n        graph=Command.PARENT,\n        update=state\n    )",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:42.007736Z",
     "start_time": "2025-04-23T08:02:41.999801Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "id": "16e5d0b2d67f49b6",
   "cell_type": "markdown",
   "source": "# Agents Definitions\n\nWe start defining the chatbot agent:\n\nDefines a stateful chatbot for generating phrases based on user-selected verbs and tenses.\nIt uses Google's generative AI model and incorporates tools for interacting with the user\n(e.g., adding/clearing verbs and tenses, confirming generation) and external functionalities\n(e.g., checking if a word is a verb, getting verb tenses, transferring to a sentence expert).\n\nThe chatbot's behavior is managed by a state graph, routing user input and model responses\nthrough different nodes: the chatbot itself, a human interaction node, and a tool execution node.\n\nKey functionalities include:\n- Engaging in conversation about verb tenses and meanings.\n- Allowing users to build a \"notebook\" of verbs and tenses.\n- Validating user input (verifying verbs and tenses).\n- Confirming the generation request with the user before proceeding.\n- Potentially delegating phrase generation to a \"sentence expert\" tool.\n\nThe chatbot starts with a welcome message and continues the conversation until the user quits.",
   "metadata": {}
  },
  {
   "id": "63b4534edc050f97",
   "cell_type": "code",
   "source": "from langchain_core.messages import AIMessage\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing_extensions import Literal\n\nCHATBOT_SYSINT = (\n    \"system\",  # 'system' indicates the message is a system instruction.\n    \"You are a Chatbot of an interactive phrase generator system. You can talk in English.\"\n    \"A human will talk to you about the different tenses that a verb can have and you will answer any questions about verb tenses \"\n    \"(and only about verb tenses - no off-topic discussion, but you can chat about the verb meaning). \"\n    \"The user will choose verbs to generate phrases for, and the tenses he wants. \"\n    \"After that you will generate phrases for those verbs in that tenses.\"\n    \"You cannot generate phrases if you do not have at least one verb and one tense.\"\n    \"\\n\\n\"\n    \"Add the chosen verb to the user notebook with add_verb, and reset the verbs with clear_verb.\"\n    \"Add tenses to the user notebook with add_tense, and reset the tenses with clear_tenses. \"\n    \"To see the contents of the notebook so far, call get_notebook (this is shown to you, not the user) \"\n    \"Always confirm_generation with the user (double-check) before calling generate_phrases. Calling confirm_generation will \"\n    \"display the selected items to the user and returns their response to seeing the list. Their response may contain modifications. \"\n    \"Always verify and respond with tenses from the GRAMMAR before adding them to the notebook. \"\n    \"Always verify for each chosen word if it's a verb with is_verb.\"\n    \"You can only generate phrases starting from verbs. \"\n    \"Once the user has finished composing its notebook, Call confirm_generation to ensure it is correct then make \"\n    \"any necessary updates.\"\n    \"You can ask the sentence expert for help with generating phrases.\"\n    \"\\n\\n\"\n    \"Do not mention the tools name in responses.\"\n    \"If any of the tools are unavailable, you can break the fourth wall and tell the user that \"\n    \"they have not implemented them yet and should keep reading to do so.\",\n)\n\nWELCOME_MSG = \"Welcome to the PhraseGenerator dojo. Type `q` to quit. What phrases can I generate today?\"\n\ntools = [get_verb_tenses, is_verb, transfer_to_sentence_expert]\ncollection_tools = [add_verb, clear_verbs, add_tense, clear_tenses, get_notebook, confirm_generation]\n\n\ndef make_chatbot(llm: ChatGoogleGenerativeAI):\n    agent_tools = tools + collection_tools\n    llm_with_tools = llm.bind_tools(agent_tools)\n    agent_tools_node = ToolNode(agent_tools)\n\n    def maybe_route_to_tools(state: PhraseGeneratorState) -> str:\n        \"\"\"Route between chat and tool nodes if a tool call is made.\"\"\"\n        if not (msgs := state.get(\"messages\", [])):\n            raise ValueError(f\"No messages found when parsing state: {state}\")\n\n        msg = msgs[-1]\n\n        if state.get(\"finished\", False):\n            # When an order is placed, exit the app. The system instruction indicates\n            # that the chatbot should say thanks and goodbye at this point, so we can exit\n            # cleanly.\n            return END\n\n        elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n            # Route to `tools` node for any automated tool calls first.\n            if msg.tool_calls[-1][\"name\"] in ToolNode(collection_tools).tools_by_name.keys():\n                return \"collector\"\n            else:\n                return \"tools\"\n\n        else:\n            return \"human\"\n\n    def chatbot(state: PhraseGeneratorState) -> PhraseGeneratorState:\n        \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n\n        if state[\"messages\"]:\n            # If there are messages, continue the conversation with the Gemini model.\n            new_output = llm_with_tools.invoke([CHATBOT_SYSINT] + state[\"messages\"])\n        else:\n            # If there are no messages, start with the welcome message.\n            new_output = AIMessage(content=WELCOME_MSG)\n\n        return state | {\"messages\": [new_output]}\n\n    def human_node(state: PhraseGeneratorState) -> PhraseGeneratorState:\n        \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n        last_msg = state[\"messages\"][-1]\n        print(\"Model:\", last_msg.content)\n\n        user_input = input(\"User: \")\n\n        # If it looks like the user is trying to quit, flag the conversation\n        # as over.\n        if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n            state[\"finished\"] = True\n\n        return state | {\"messages\": [(\"user\", user_input)]}\n\n    def maybe_exit_human_node(state: PhraseGeneratorState) -> Literal[\"chatbot\", \"__end__\"]:\n        \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n        if state.get(\"finished\", False):\n            return END\n        else:\n            return \"chatbot\"\n\n    # Start building a new graph.\n    graph_builder = StateGraph(PhraseGeneratorState)\n\n    # Add the chatbot and human nodes to the app graph.\n    graph_builder.add_node(\"chatbot\", chatbot)\n    graph_builder.add_node(\"human\", human_node)\n    graph_builder.add_node(\"tools\", agent_tools_node)\n    graph_builder.add_node(\"collector\", collector_node)\n\n    # Chatbot may go to tools, or human.\n    graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n    # Human may go back to chatbot, or exit.\n    graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n\n    # Tools always route back to chat afterwards.\n    graph_builder.add_edge(\"tools\", \"chatbot\")\n    graph_builder.add_edge(\"collector\", \"chatbot\")\n\n    graph_builder.add_edge(START, \"chatbot\")\n\n    return graph_builder.compile()\n",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:42.023308Z",
     "start_time": "2025-04-23T08:02:42.016307Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "id": "e0983f849f29adfb",
   "cell_type": "markdown",
   "source": "Defines a \"Sentence Expert\" chatbot focused on generating sentences based on user requests.\nIt utilizes Google's generative AI model and has access to specific tools:\n- `generate_sentences`: The primary tool for creating the desired phrases.\n- `transfer_to_chatbot`: A tool to hand the conversation back to the main chatbot\n                         after sentence generation is complete.\n\nThe Sentence Expert operates using a state graph to manage the flow of conversation\nand tool calls. It includes nodes for:\n- `expert`: The core logic of the sentence expert, interacting with the LLM.\n- `human`: Handling user input and displaying the model's responses.\n- `tools`: Executing general utility tools (in this case, only `transfer_to_chatbot`).\n- `generator`: Specifically executing the `generate_sentences` tool.\n\nThe conversation flow involves the expert receiving instructions, potentially calling\n`generate_sentences`, and then using `transfer_to_chatbot` to return control to the\nmain chatbot. The system instruction guides the expert to primarily focus on sentence\ngeneration and then hand off the conversation.",
   "metadata": {}
  },
  {
   "id": "8b4d77703b8b4c8a",
   "cell_type": "code",
   "source": "from typing import Literal\n\nfrom langchain_core.messages import SystemMessage\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langgraph.constants import START, END\nfrom langgraph.prebuilt import ToolNode\n\nSENTENCE_EXPERT_SYSINT = \"\"\"\n    You are a Sentence Expert.\n    To generate sentences, you can call generate_sentences\n    After the generation, you should transfer the conversation to the chatbot.\n    \\n\\n\n    If any of the tools are unavailable, you can break the fourth wall and tell the user that\n    they have not implemented them yet and should keep reading to do so.\n    \"\"\"\n\ntools = [transfer_to_chatbot]\ngenerator_tools = [generate_sentences]\n\ndef make_sentence_expert(llm: ChatGoogleGenerativeAI):\n    agent_tools = tools + generator_tools\n    llm_with_tools = llm.bind_tools(agent_tools)\n    agent_tools_node = ToolNode(agent_tools)\n\n    def maybe_route_to_tools(state: PhraseGeneratorState) -> str:\n        \"\"\"Route between chat and tool nodes if a tool call is made.\"\"\"\n        if not (msgs := state.get(\"messages\", [])):\n            raise ValueError(f\"No messages found when parsing state: {state}\")\n\n        msg = msgs[-1]\n\n        if state.get(\"finished\", False):\n            # When an order is placed, exit the app. The system instruction indicates\n            # that the chatbot should say thanks and goodbye at this point, so we can exit\n            # cleanly.\n            return END\n\n        elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n            # Route to `tools` node for any automated tool calls first.\n            if msg.tool_calls[-1][\"name\"] in ToolNode(generator_tools).tools_by_name.keys():\n                return \"generator\"\n            else:\n                return \"tools\"\n\n        else:\n            return \"human\"\n\n    def expert(state: PhraseGeneratorState) -> PhraseGeneratorState:\n        \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n\n        system_message = SystemMessage(content=SENTENCE_EXPERT_SYSINT)\n        new_output = llm_with_tools.invoke([system_message] + state[\"messages\"])\n\n        return state | {\"messages\": [new_output]}\n\n    def human_node(state: PhraseGeneratorState) -> PhraseGeneratorState:\n        \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n        last_msg = state[\"messages\"][-1]\n        print(\"Model:\", last_msg.content)\n\n        user_input = input(\"User: \")\n\n        # If it looks like the user is trying to quit, flag the conversation\n        # as over.\n        if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n            state[\"finished\"] = True\n\n        return state | {\"messages\": [(\"user\", user_input)]}\n\n    def maybe_exit_human_node(state: PhraseGeneratorState) -> Literal[\"expert\", \"__end__\"]:\n        \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n        if state.get(\"finished\", False):\n            return END\n        else:\n            return \"expert\"\n\n    # Start building a new graph.\n    graph_builder = StateGraph(PhraseGeneratorState)\n\n    # Add the chatbot and human nodes to the app graph.\n    graph_builder.add_node(\"expert\", expert)\n    graph_builder.add_node(\"human\", human_node)\n    graph_builder.add_node(\"tools\", agent_tools_node)\n    graph_builder.add_node(\"generator\", generator_node)\n\n    # Chatbot may go to tools, or human.\n    graph_builder.add_conditional_edges(\"expert\", maybe_route_to_tools)\n    # Human may go back to chatbot, or exit.\n    graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n\n    # Tools always route back to chat afterwards.\n    graph_builder.add_edge(\"tools\", \"expert\")\n    graph_builder.add_edge(\"generator\", \"expert\")\n\n    graph_builder.add_edge(START, \"expert\")\n\n    return graph_builder.compile()",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:02:42.039441Z",
     "start_time": "2025-04-23T08:02:42.031891Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "id": "75df0a6dba4ddff8",
   "cell_type": "markdown",
   "source": "# Code to run the agents\n\nThis code sets up the main workflow for the phrase generation system.\nIt initializes two separate chatbots using Google's Gemini Flash model:\n- `chatbot`: The primary interactive bot for discussing verb tenses and collecting\n             user preferences for verbs and tenses.\n- `sentence_expert`: A specialized bot dedicated to generating the actual phrases\n                     based on the information gathered by the main chatbot.\n\nA LangGraph `StateGraph` is created to define the flow between these two chatbots.\nCurrently, it's a simple graph with:\n- A \"chatbot\" node running the main chatbot logic.\n- A \"sentence_expert\" node running the sentence generation expert logic.\n- An edge connecting the start of the graph to the \"chatbot\", meaning the\n  interaction begins with the main chatbot.\n\nThe graph is compiled into an executable object.\n\nFinally, the code invokes the graph with an initial empty message list, starting\nthe conversation with the main chatbot. A recursion limit is set for the graph execution.",
   "metadata": {}
  },
  {
   "id": "3b95ce805fc35424",
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "chatbot = make_chatbot(ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\"))\n",
    "sentence_expert = make_sentence_expert(ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\"))\n",
    "\n",
    "builder = StateGraph(PhraseGeneratorState)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_node(\"sentence_expert\", sentence_expert)\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "graph = builder.compile()\n",
    "\n",
    "config = {\"recursion_limit\": 100}\n",
    "\n",
    "# Uncomment this line to execute the graph:\n",
    "state = graph.invoke({\"messages\": []}, config)\n",
    "\n",
    "# Example conversation (only human parts)\n",
    "# - What tenses do you support?\n",
    "# - Can you generate a sentence with the verb 食べる at the imperative tense?\n",
    "# - Can you add verbs 食べる、見る and 写す?\n",
    "# - Can you add verbs 走る and 今日?\n",
    "# - What there is my notebook?\n",
    "# - add Request, volitional, past negative polite and conditional\n",
    "# - add non past tense"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:03:11.661672Z",
     "start_time": "2025-04-23T08:02:42.049977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Welcome to the PhraseGenerator dojo. Type `q` to quit. What phrases can I generate today?\n",
      "Your notebook:\n",
      "Verbs:\n",
      "   食べる\n",
      "Tenses:\n",
      "   imperative\n",
      "Model: I understand that you want me to generate a phrase with the verb 食べる at the imperative tense. Is that correct?\n",
      "Model: OK. Please wait while I generate the phrase.\n",
      "Model: If you want to generate more phrases, tell me the verb and the tense, or type `q` to quit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     14\u001B[39m config = {\u001B[33m\"\u001B[39m\u001B[33mrecursion_limit\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m100\u001B[39m}\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Uncomment this line to execute the graph:\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m state = \u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Example conversation (only human parts)\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# - What tenses do you support?\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# - Can you generate a sentence with the verb 食べる at the imperative tense?\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# - add Request, volitional, past negative polite and conditional\u001B[39;00m\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# - add non past tense\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2718\u001B[39m, in \u001B[36mPregel.invoke\u001B[39m\u001B[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001B[39m\n\u001B[32m   2716\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2717\u001B[39m     chunks = []\n\u001B[32m-> \u001B[39m\u001B[32m2718\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2719\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2720\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2721\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2722\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2723\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2724\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2725\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_during\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheckpoint_during\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2726\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2727\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2728\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2729\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mvalues\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[32m   2730\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlatest\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2356\u001B[39m, in \u001B[36mPregel.stream\u001B[39m\u001B[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001B[39m\n\u001B[32m   2350\u001B[39m     \u001B[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001B[39;00m\n\u001B[32m   2351\u001B[39m     \u001B[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001B[39;00m\n\u001B[32m   2352\u001B[39m     \u001B[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001B[39;00m\n\u001B[32m   2353\u001B[39m     \u001B[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001B[39;00m\n\u001B[32m   2354\u001B[39m     \u001B[38;5;66;03m# with channel updates applied only at the transition between steps.\u001B[39;00m\n\u001B[32m   2355\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m loop.tick(input_keys=\u001B[38;5;28mself\u001B[39m.input_channels):\n\u001B[32m-> \u001B[39m\u001B[32m2356\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrunner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2357\u001B[39m \u001B[43m            \u001B[49m\u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2358\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2359\u001B[39m \u001B[43m            \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2360\u001B[39m \u001B[43m            \u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2361\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2362\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# emit output\u001B[39;49;00m\n\u001B[32m   2363\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield from\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2364\u001B[39m \u001B[38;5;66;03m# emit output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\runner.py:158\u001B[39m, in \u001B[36mPregelRunner.tick\u001B[39m\u001B[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001B[39m\n\u001B[32m    156\u001B[39m t = tasks[\u001B[32m0\u001B[39m]\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[43mrun_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfigurable\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m            \u001B[49m\u001B[43mCONFIG_KEY_CALL\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m                \u001B[49m\u001B[43m_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m                \u001B[49m\u001B[43mweakref\u001B[49m\u001B[43m.\u001B[49m\u001B[43mref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m                \u001B[49m\u001B[43mretry\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    166\u001B[39m \u001B[43m                \u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweakref\u001B[49m\u001B[43m.\u001B[49m\u001B[43mref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    167\u001B[39m \u001B[43m                \u001B[49m\u001B[43mschedule_task\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mschedule_task\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    168\u001B[39m \u001B[43m                \u001B[49m\u001B[43msubmit\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubmit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[43m                \u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    173\u001B[39m     \u001B[38;5;28mself\u001B[39m.commit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\retry.py:39\u001B[39m, in \u001B[36mrun_with_retry\u001B[39m\u001B[34m(task, retry_policy, configurable)\u001B[39m\n\u001B[32m     37\u001B[39m     task.writes.clear()\n\u001B[32m     38\u001B[39m     \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43mproc\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ParentCommand \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m     41\u001B[39m     ns: \u001B[38;5;28mstr\u001B[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\utils\\runnable.py:622\u001B[39m, in \u001B[36mRunnableSeq.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    620\u001B[39m     \u001B[38;5;66;03m# run in context\u001B[39;00m\n\u001B[32m    621\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(config, run) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[32m--> \u001B[39m\u001B[32m622\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    623\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    624\u001B[39m     \u001B[38;5;28minput\u001B[39m = step.invoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2718\u001B[39m, in \u001B[36mPregel.invoke\u001B[39m\u001B[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001B[39m\n\u001B[32m   2716\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2717\u001B[39m     chunks = []\n\u001B[32m-> \u001B[39m\u001B[32m2718\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2719\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2720\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2721\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2722\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2723\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2724\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2725\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_during\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheckpoint_during\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2726\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2727\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2728\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2729\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mvalues\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[32m   2730\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlatest\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2356\u001B[39m, in \u001B[36mPregel.stream\u001B[39m\u001B[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001B[39m\n\u001B[32m   2350\u001B[39m     \u001B[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001B[39;00m\n\u001B[32m   2351\u001B[39m     \u001B[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001B[39;00m\n\u001B[32m   2352\u001B[39m     \u001B[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001B[39;00m\n\u001B[32m   2353\u001B[39m     \u001B[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001B[39;00m\n\u001B[32m   2354\u001B[39m     \u001B[38;5;66;03m# with channel updates applied only at the transition between steps.\u001B[39;00m\n\u001B[32m   2355\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m loop.tick(input_keys=\u001B[38;5;28mself\u001B[39m.input_channels):\n\u001B[32m-> \u001B[39m\u001B[32m2356\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrunner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2357\u001B[39m \u001B[43m            \u001B[49m\u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2358\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2359\u001B[39m \u001B[43m            \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2360\u001B[39m \u001B[43m            \u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2361\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2362\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# emit output\u001B[39;49;00m\n\u001B[32m   2363\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield from\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2364\u001B[39m \u001B[38;5;66;03m# emit output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\runner.py:158\u001B[39m, in \u001B[36mPregelRunner.tick\u001B[39m\u001B[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001B[39m\n\u001B[32m    156\u001B[39m t = tasks[\u001B[32m0\u001B[39m]\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[43mrun_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfigurable\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m            \u001B[49m\u001B[43mCONFIG_KEY_CALL\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m                \u001B[49m\u001B[43m_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m                \u001B[49m\u001B[43mweakref\u001B[49m\u001B[43m.\u001B[49m\u001B[43mref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m                \u001B[49m\u001B[43mretry\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    166\u001B[39m \u001B[43m                \u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweakref\u001B[49m\u001B[43m.\u001B[49m\u001B[43mref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    167\u001B[39m \u001B[43m                \u001B[49m\u001B[43mschedule_task\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mschedule_task\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    168\u001B[39m \u001B[43m                \u001B[49m\u001B[43msubmit\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubmit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[43m                \u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    173\u001B[39m     \u001B[38;5;28mself\u001B[39m.commit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\pregel\\retry.py:39\u001B[39m, in \u001B[36mrun_with_retry\u001B[39m\u001B[34m(task, retry_policy, configurable)\u001B[39m\n\u001B[32m     37\u001B[39m     task.writes.clear()\n\u001B[32m     38\u001B[39m     \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43mproc\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ParentCommand \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m     41\u001B[39m     ns: \u001B[38;5;28mstr\u001B[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\utils\\runnable.py:622\u001B[39m, in \u001B[36mRunnableSeq.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    620\u001B[39m     \u001B[38;5;66;03m# run in context\u001B[39;00m\n\u001B[32m    621\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(config, run) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[32m--> \u001B[39m\u001B[32m622\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    623\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    624\u001B[39m     \u001B[38;5;28minput\u001B[39m = step.invoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\langgraph\\utils\\runnable.py:376\u001B[39m, in \u001B[36mRunnableCallable.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    374\u001B[39m         run_manager.on_chain_end(ret)\n\u001B[32m    375\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m376\u001B[39m     ret = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.recurse \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable):\n\u001B[32m    378\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ret.invoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 82\u001B[39m, in \u001B[36mmake_chatbot.<locals>.human_node\u001B[39m\u001B[34m(state)\u001B[39m\n\u001B[32m     79\u001B[39m last_msg = state[\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m][-\u001B[32m1\u001B[39m]\n\u001B[32m     80\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mModel:\u001B[39m\u001B[33m\"\u001B[39m, last_msg.content)\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m user_input = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mUser: \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     84\u001B[39m \u001B[38;5;66;03m# If it looks like the user is trying to quit, flag the conversation\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[38;5;66;03m# as over.\u001B[39;00m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m user_input \u001B[38;5;129;01min\u001B[39;00m {\u001B[33m\"\u001B[39m\u001B[33mq\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mquit\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mexit\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mgoodbye\u001B[39m\u001B[33m\"\u001B[39m}:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1280\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1281\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1282\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1283\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1284\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1285\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1286\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1287\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\SenseiBot\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1322\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1323\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1324\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1325\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1326\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1327\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "id": "3514621e24ffc656",
   "cell_type": "code",
   "source": "",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:03:11.663830500Z",
     "start_time": "2025-04-20T08:25:35.340153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
